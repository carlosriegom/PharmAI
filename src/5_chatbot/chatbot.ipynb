{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chatbot con `Llama2-7b`**\n",
    "Construimos un assitente farmacéutico a modo de chatbot, con el modelo `Llama 2-7b`de `Meta`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/PharmAI/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# utils\n",
    "# Agrega la ruta del directorio donde está el utils al path de Python\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from utils import load_json\n",
    "\n",
    "# from sklearn.decomposition import uPCA\n",
    "import seaborn as sns\n",
    "import faiss\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **1. Cargar el modelo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### **`Llama2-7b` (normal o chat)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.12s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "def load_llama_model():\n",
    "\n",
    "    # Detectar el dispositivo disponible: CUDA, MPS (para Mac con Apple Silicon) o CPU\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    print(\"Usando dispositivo:\", device)\n",
    "\n",
    "    # Nombre del modelo a cargar (Llama-2-7b)\n",
    "    #model_name = \"meta-llama/Llama-2-7b-hf\" # Llama2 normal\n",
    "    model_name = \"meta-llama/Llama-2-7b-chat-hf\" # Llama2 chat\n",
    "    \n",
    "    # Cargar el tokenizador\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "    # Cargar el modelo, especificando el tipo de datos y usando device_map=\"auto\" para aprovechar la GPU\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if device in [\"cuda\", \"mps\"] else torch.float32,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "# Cargar el modelo\n",
    "model, tokenizer = load_llama_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### **`GPT2`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gpt2_model():\n",
    "    # Detectar el dispositivo: MPS para Mac con Apple Silicon o CPU\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(\"Usando dispositivo:\", device)\n",
    "\n",
    "    model_name = \"gpt2\"  # Puedes cambiar a \"gpt2-medium\" o \"gpt2-large\" si lo deseas\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Mover el modelo al dispositivo seleccionado\n",
    "    model.to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Cargar el modelo GPT-2\n",
    "#model, tokenizer = load_gpt2_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### **`GPT-J-6b`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gpt_j_model():\n",
    "    # Detectar el dispositivo disponible: CUDA, MPS (para Mac con Apple Silicon) o CPU\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    print(\"Usando dispositivo:\", device)\n",
    "\n",
    "    # Nombre del modelo a cargar (GPT-J-6B)\n",
    "    model_name = \"EleutherAI/gpt-j-6B\"\n",
    "\n",
    "    # Cargar el tokenizador\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "    # Cargar el modelo, especificando el tipo de datos y usando device_map=\"auto\" para aprovechar la GPU\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if device in [\"cuda\", \"mps\"] else torch.float32,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "# Cargar el modelo\n",
    "#model, tokenizer = load_gpt_j_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **2. Funciones para buscar el contexto en la BBDD vectorial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_fragments_prueba(query, model, fragments, index, k=5):\n",
    "    \"\"\"\n",
    "    Recupera los fragmentos más relevantes para la consulta utilizando un modelo de similitud (ej. FAISS).\n",
    "\n",
    "    Parámetros:\n",
    "    - query (str): Consulta del usuario.\n",
    "    - top_k (int): Número de fragmentos a recuperar.\n",
    "\n",
    "    Retorna:\n",
    "    - list: Lista de fragmentos relevantes.\n",
    "    \"\"\"\n",
    "    # Simulación de la búsqueda, usando FAISS o similar.\n",
    "    # Esto debería ser reemplazado por la implementación real que recupera los fragmentos relevantes.\n",
    "    # Suponemos que \"retrieved_fragments\" es el resultado de una búsqueda en base de datos vectorial.\n",
    "    '''\n",
    "    retrieved_fragments = [\n",
    "        {\n",
    "            \"medicamento\": \"Aspirina\",\n",
    "            \"categoria\": \"efectos_secundarios\",\n",
    "            \"texto\": \"Puede causar náuseas y dolor de estómago.\",\n",
    "        },\n",
    "        {\n",
    "            \"medicamento\": \"Paracetamol\",\n",
    "            \"categoria\": \"efectos_secundarios\",\n",
    "            \"texto\": \"Puede causar problemas hepáticos en dosis altas.\",\n",
    "        },\n",
    "    ]\n",
    "    '''\n",
    "    retrieved_fragments = [\n",
    "        {\n",
    "            \"medicamento\": \"Paracetamol\",\n",
    "            \"categoria\": \"efectos_secundarios\",\n",
    "            \"texto\": \"Puede causar problemas hepáticos en dosis altas.\",\n",
    "        }\n",
    "    ]\n",
    "    return retrieved_fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_fragments(query, embedding_model, fragments, index, k=10):\n",
    "    \"\"\"\n",
    "    Realiza una búsqueda en FAISS para encontrar los fragmentos más similares a la consulta.\n",
    "\n",
    "    Parámetros:\n",
    "    - query (str): La consulta en lenguaje natural.\n",
    "    - k (int): Número de resultados a recuperar.\n",
    "\n",
    "    Retorna:\n",
    "    - Lista de fragmentos de texto relevantes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convertir la consulta en embedding\n",
    "    query_embedding = embedding_model.encode(query, convert_to_numpy=True).reshape(1, -1)\n",
    "\n",
    "    # Buscar los k embeddings más cercanos\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "    # Recuperar los fragmentos correspondientes, incluyendo las distancias\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx < len(fragments):  # Asegurar que el índice es válido\n",
    "            results.append(\n",
    "                {\n",
    "                    **fragments[idx],  # Añadir los datos del fragmento\n",
    "                    \"distance\": distances[0][i],  # Añadir la distancia de similitud\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context(retrieved_fragments, max_fragments=5, max_text_length=2000):\n",
    "    \"\"\"\n",
    "    Formatea los fragmentos recuperados en un contexto para el modelo. Transforma una lista de diccionarios en un texto estructurado.\n",
    "\n",
    "    Parámetros:\n",
    "    - retrieved_fragments (list): Lista de fragmentos recuperados (diccionarios)\n",
    "    - max_fragments (int): Número máximo de fragmentos a utilizar\n",
    "    - max_text_length (int): Longitud máxima del texto a mostrar por fragmento\n",
    "\n",
    "    Retorna:\n",
    "    - str: Contexto formateado para el modelo\n",
    "    \"\"\"\n",
    "    context = \"\"\n",
    "    \n",
    "    # Asegurar que no se intenten tomar más fragmentos de los que existen\n",
    "    num_fragments = min(len(retrieved_fragments), max_fragments)\n",
    "\n",
    "    for i, frag in enumerate(retrieved_fragments[:num_fragments]):\n",
    "        # Verificar que cada fragmento tenga las claves necesarias\n",
    "        if not all(key in frag for key in [\"medicamento\", \"categoria\", \"texto\"]):\n",
    "            print(f\"Advertencia: Fragmento {i+1} no tiene la estructura esperada.\")\n",
    "            continue  # Saltar fragmentos mal formateados\n",
    "\n",
    "        medicamento = frag[\"medicamento\"]\n",
    "        categoria = frag[\"categoria\"]\n",
    "        texto = frag[\"texto\"]\n",
    "\n",
    "        # Limitar la longitud del texto\n",
    "        truncated_text = (\n",
    "            texto[:max_text_length] + \"...\" if len(texto) > max_text_length else texto\n",
    "        )\n",
    "\n",
    "        # Construcción del contexto\n",
    "        context += f\"\\nFragmento {i+1}:\\n\"\n",
    "        context += f\"Medicamento: {medicamento}\\n\"\n",
    "        context += f\"Categoría: {categoria}\\n\"\n",
    "        context += f\"Información: {truncated_text}\\n\"\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **3. Función para construir el Prompt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Prompt resumido**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef build_prompt(context, query):\\n    \"\"\"\\n    Construye el prompt para el modelo con base en el contexto y la consulta,\\n    incluyendo un ejemplo de cómo debe formatear la respuesta.\\n\\n    Parámetros:\\n    - context (str): Contexto a proporcionar al modelo\\n    - query (str): Consulta del usuario\\n\\n    Retorna:\\n    - str: Prompt completo para el modelo\\n    \"\"\"\\n    prompt = f\"\"\"Eres un asistente médico especializado en información sobre medicamentos.\\n    Basándote ÚNICAMENTE en la siguiente información sobre medicamentos:\\n\\n    {context}\\n\\n    Responde de manera clara y precisa a esta pregunta: {query}\\n\\n    Ejemplo de consulta:\\n    Pregunta: ¿Cuáles son los efectos secundarios de la aspirina?\\n    Contexto: \\n    Fragmento 1:\\n    Medicamento: Aspirina\\n    Categoría: reacciones_adversas\\n    Información: Los efectos secundarios comunes incluyen náuseas, dolor de estómago y sangrados.\\n\\n    Respuesta:\\n    La aspirina puede causar efectos secundarios como náuseas, dolor de estómago y sangrados. Si necesitas más detalles, por favor consulta la ficha técnica completa.\\n\\n    Si la información proporcionada no es suficiente para responder completamente, indica qué datos faltan, pero no inventes información ni dejes la respuesta vacía.\\n\\n    Tu respuesta debe ser:\\n    1. Precisa y basada solo en el contexto proporcionado.\\n    2. Estructurada y fácil de entender.\\n    3. Sin añadir información que no esté en los fragmentos.\\n    4. Con referencias claras al medicamento mencionado.\\n\\n    Respuesta:\"\"\"\\n    return prompt\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def build_prompt(context, query):\n",
    "    \"\"\"\n",
    "    Construye el prompt para el modelo con base en el contexto y la consulta,\n",
    "    incluyendo un ejemplo de cómo debe formatear la respuesta.\n",
    "\n",
    "    Parámetros:\n",
    "    - context (str): Contexto a proporcionar al modelo\n",
    "    - query (str): Consulta del usuario\n",
    "\n",
    "    Retorna:\n",
    "    - str: Prompt completo para el modelo\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Eres un asistente médico especializado en información sobre medicamentos.\n",
    "    Basándote ÚNICAMENTE en la siguiente información sobre medicamentos:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Responde de manera clara y precisa a esta pregunta: {query}\n",
    "\n",
    "    Ejemplo de consulta:\n",
    "    Pregunta: ¿Cuáles son los efectos secundarios de la aspirina?\n",
    "    Contexto: \n",
    "    Fragmento 1:\n",
    "    Medicamento: Aspirina\n",
    "    Categoría: reacciones_adversas\n",
    "    Información: Los efectos secundarios comunes incluyen náuseas, dolor de estómago y sangrados.\n",
    "\n",
    "    Respuesta:\n",
    "    La aspirina puede causar efectos secundarios como náuseas, dolor de estómago y sangrados. Si necesitas más detalles, por favor consulta la ficha técnica completa.\n",
    "\n",
    "    Si la información proporcionada no es suficiente para responder completamente, indica qué datos faltan, pero no inventes información ni dejes la respuesta vacía.\n",
    "\n",
    "    Tu respuesta debe ser:\n",
    "    1. Precisa y basada solo en el contexto proporcionado.\n",
    "    2. Estructurada y fácil de entender.\n",
    "    3. Sin añadir información que no esté en los fragmentos.\n",
    "    4. Con referencias claras al medicamento mencionado.\n",
    "\n",
    "    Respuesta:\"\"\"\n",
    "    return prompt\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Prompt mejorado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(context, query):\n",
    "    \"\"\"\n",
    "    Construye el prompt para el modelo con base en el contexto y la consulta,\n",
    "    incluyendo un ejemplo de cómo debe formatear la respuesta.\n",
    "\n",
    "    Parámetros:\n",
    "    - context (str): Contexto a proporcionar al modelo\n",
    "    - query (str): Consulta del usuario\n",
    "\n",
    "    Retorna:\n",
    "    - str: Prompt completo para el modelo\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    \n",
    "    1. OBJETIVO GENERAL:\n",
    "    Eres un asistente médico especializado en información sobre medicamentos. Debes responder a la pregunta del usuario basándote únicamente en la información proporcionada. No debes inventar ni suponer información adicional.\n",
    "\n",
    "    2. FORMATO DEL CONTEXTO:\n",
    "    El contexto se presenta como un texto con varios fragmentos que contiene información de uno o varios medicamentos presentes en la pregunta del ususario, donde cada fragmento tiene el siguiente formato:\n",
    "    - Medicamento: Nombre del medicamento\n",
    "    - Categoría: Categoría de la información (ej. efectos secundarios, interacciones)\n",
    "    - Información: Texto relevante sobre el medicamento, el cual debes analizar antes de responder.\n",
    "\n",
    "    3. FORMATO DE RESPUESTA:\n",
    "    - Debes responder de manera clara y precisa a la pregunta formulada por el usuario, utilizando ÚNICAMENTE el contexto que se te está proporcionando. \n",
    "    - Si la información proporcionada no es suficiente para responder completamente, indica qué datos faltan.\n",
    "    - Incluye una mención explícita a los textos que respaldan tu respuesta, indicando para todos ellos el medicamento y la categoría.\n",
    "\n",
    "    4. EJEMPLOS DE CONSULTA Y DE RESPUESTA:\n",
    "    - EJEMPLO 1:\n",
    "        Pregunta: ¿Cuáles son los efectos secundarios de la aspirina?\n",
    "        Contexto:\n",
    "            \"medicamento\": \"Aspirina\"\n",
    "            \"categoria\": \"efectos_secundarios\"\n",
    "            \"texto\": \"Puede causar náuseas y dolor de estómago.\"\n",
    "        Respuesta:\n",
    "        La aspirina puede causar efectos secundarios como náuseas y dolor de estómago (extraído de la sección \"efectos_secundarios\" del medicamento \"ASPIRINA\": \"la aspirina tiene como efectos secundarios, entre ottros, la aparición de náuseas y dolor de tripa\"). Si necesitas más detalles, por favor consulta la ficha técnica completa.\n",
    "\n",
    "    - EJEMPLO 2:\n",
    "        Pregunta: ¿Puedo tomar medicamentoA si estoy embarazada?\n",
    "        Contexto:\n",
    "            \"medicamento\": \"medicamentoA\"\n",
    "            \"categoria\": \"contraindicaciones\"\n",
    "            \"texto\": \"No se recomienda su uso durante el embarazo.\"\n",
    "        Respuesta:\n",
    "        No se recomienda el uso de medicamentoA durante el embarazo (extraído de la sección \"fertilidad_embarazo\" del medicamento \"medicamentoA\": \"el uso de medicamentoA durante el embarazo puede entrañar riesgos, por lo que se desaconseja totalmente su uso en embarazadas\"). Si necesitas más detalles, por favor consulta la ficha técnica completa.\n",
    "\n",
    "    5. INSTRUCCIONES FINALES:\n",
    "    Básandote ÚNICAMENTE en la información proporcionada en ({context}), responde a la siguiente pregunta:\n",
    "    {query}\n",
    "\n",
    "    6. RECUERDA:\n",
    "    - No debes inventar información ni suponer datos que no estén presentes en el contexto.\n",
    "    - Si es posible, referencia el fragmento específico que respalda tu respuesta, indicando el medicamento y la categoría.\n",
    "    - Si la información proporcionada no es suficiente para responder completamente, indica qué datos faltan.\n",
    "    - Si la pregunta no está relacionada con medicamentos, indica que no puedes ayudar en ese caso.\n",
    "\n",
    "    Respuesta:\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **4. Función para generar respuestas basadas en el contexto y el prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, context, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Genera una respuesta basada en los fragmentos recuperados usando LLaMA.\n",
    "\n",
    "    Parámetros:\n",
    "    - query (str): La consulta del usuario\n",
    "    - context (string): Texto formateado con los fragmentos recuperados \n",
    "    - tokenizer: Tokenizador del modelo\n",
    "    - model: Modelo generativo\n",
    "\n",
    "    Retorna:\n",
    "    - str: Respuesta generada\n",
    "    \"\"\"\n",
    "\n",
    "    # Construimos el prompt para el modelo\n",
    "    prompt = build_prompt(context, query)\n",
    "\n",
    "    # Tokenizamos el prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Detectar el dispositivo disponible: CUDA, MPS (para Mac con Apple Silicon) o CPU\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    print(\"Usando dispositivo:\", device)\n",
    "\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # Generamos la respuesta usando el modelo\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_length=len(input_ids[0]) + 2000,  # Limita la longitud de salida\n",
    "            do_sample=True,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "        )\n",
    "\n",
    "    # Decodificamos la respuesta generada\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extraemos solo la parte de la respuesta después del prompt\n",
    "    response = response[\n",
    "        len(prompt) :\n",
    "    ].strip()  # Ajuste para capturar la respuesta correctamente\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **5. Función para realizar la consulta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Realiza una consulta y genera una respuesta utilizando el modelo.\n",
    "\n",
    "    Parámetros:\n",
    "    - query (str): La consulta del usuario\n",
    "\n",
    "    Retorna:\n",
    "    - str: Respuesta generada\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Converir la consulta a minúsculas\n",
    "    query = query.lower()\n",
    "\n",
    "    # 2. Recuperamos los fragmentos relevantes para la consulta\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\") # old\n",
    "    fragments = load_json(\"../../data/outputs/5_chatbot/contexto_medicamentos_chatbot.json\") \n",
    "    index = faiss.read_index(\"../../data/outputs/5_chatbot/faiss_index_all-MiniLM-L6-v2.bin\") # old\n",
    "\n",
    "    # 3. Busca los fragmentos relevantes\n",
    "    #retrieved_fragments = retrieve_relevant_fragments_prueba(query, embedding_model, fragments, index, k=5)\n",
    "    retrieved_fragments = retrieve_relevant_fragments(query, embedding_model, fragments, index, k=5)\n",
    "\n",
    "    # 4. Aplicamos formateo al contexto\n",
    "    print(f\"Fragmentos recuperados: {retrieved_fragments}\")\n",
    "    context = format_context(retrieved_fragments)\n",
    "\n",
    "    # 5. Generamos la respuesta del modelo\n",
    "    print(f\"Contexto: {context}\")\n",
    "    response = generate_answer(query, context, tokenizer, model)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **6. Ejemplo de Consulta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fragmentos recuperados: [{'medicamento': 'PARACETAMOL_MABO_1_g_COMPRIMIDOS_EFG', 'categoria': 'contraindicaciones', 'texto': 'hipersensibilidad al paracetamol, o a alguno de los excipientes.', 'distance': np.float32(0.4822652)}, {'medicamento': 'ANTIDOL_NOCHE_500_MG_25_MG_COMPRIMIDOS_RECUBIERTOS_CON_PELICULA', 'categoria': 'contraindicaciones', 'texto': '- - hipersensibilidad al paracetamol, a difenhidramina o a alguno de los excipientes incluidos en la sección 6.1. porfiria.', 'distance': np.float32(0.5046341)}, {'medicamento': 'DOLOMIDINA_500_25MG_COMPRIMIDOS_RECUBIERTOS_CON_PELICULA', 'categoria': 'contraindicaciones', 'texto': '- - hipersensibilidad al paracetamol, a difenhidramina o a alguno de los excipientes incluidos en la sección 6.1. porfiria.', 'distance': np.float32(0.5046341)}, {'medicamento': 'DOLOSTOP_500_MG_COMPRIMIDOS', 'categoria': 'contraindicaciones', 'texto': 'hipersensibilidad al paracetamol o a alguno de los excipientes.', 'distance': np.float32(0.505387)}, {'medicamento': 'FEBRECTAL_NIÑOS_300_MG_SUPOSITORIOS', 'categoria': 'contraindicaciones', 'texto': 'hipersensibilidad al paracetamol o a alguno de los excipientes.', 'distance': np.float32(0.505387)}]\n",
      "Contexto: \n",
      "Fragmento 1:\n",
      "Medicamento: PARACETAMOL_MABO_1_g_COMPRIMIDOS_EFG\n",
      "Categoría: contraindicaciones\n",
      "Información: hipersensibilidad al paracetamol, o a alguno de los excipientes.\n",
      "\n",
      "Fragmento 2:\n",
      "Medicamento: ANTIDOL_NOCHE_500_MG_25_MG_COMPRIMIDOS_RECUBIERTOS_CON_PELICULA\n",
      "Categoría: contraindicaciones\n",
      "Información: - - hipersensibilidad al paracetamol, a difenhidramina o a alguno de los excipientes incluidos en la sección 6.1. porfiria.\n",
      "\n",
      "Fragmento 3:\n",
      "Medicamento: DOLOMIDINA_500_25MG_COMPRIMIDOS_RECUBIERTOS_CON_PELICULA\n",
      "Categoría: contraindicaciones\n",
      "Información: - - hipersensibilidad al paracetamol, a difenhidramina o a alguno de los excipientes incluidos en la sección 6.1. porfiria.\n",
      "\n",
      "Fragmento 4:\n",
      "Medicamento: DOLOSTOP_500_MG_COMPRIMIDOS\n",
      "Categoría: contraindicaciones\n",
      "Información: hipersensibilidad al paracetamol o a alguno de los excipientes.\n",
      "\n",
      "Fragmento 5:\n",
      "Medicamento: FEBRECTAL_NIÑOS_300_MG_SUPOSITORIOS\n",
      "Categoría: contraindicaciones\n",
      "Información: hipersensibilidad al paracetamol o a alguno de los excipientes.\n",
      "\n",
      "Usando dispositivo: mps\n",
      "Respuesta generada: Los efectos secundarios del paracetamol pueden ser: hipersensibilidad al paracetamol, o a alguno de los excipientes. Por favor, consulte la ficha técnica completa para obtener más información.\n"
     ]
    }
   ],
   "source": [
    "#query = \"¿Cuáles son las reacciones adversas del paracetamol?\"\n",
    "query = \"¿Cuáles son los efectos a la hora de conducir del ibuprofeno?\" # Funciona bien\n",
    "query = \"¿Cuáles son las reacciones adversas del ibuprofeno?\"response = answer_query(query, model, tokenizer)\n",
    "print(\"Respuesta generada:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PharmAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
